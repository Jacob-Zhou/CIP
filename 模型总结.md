# 最大匹配分词

最大分词匹配是分词问题的最简单的模型。

首先构建一个词典。（本实验中是直接通过已经分好的词来得到词典，有点作弊的意思）

假设词典中最长的词长度为max_len。

在对一句话进行分词时，先切分出max_len长度的词，若不在词典中，则缩短词长，继续匹配。这样一个一个匹配字典中最长的词。

最大匹配分词有两种方向，前向和后向。一般来说对于中文，后向的效果要比前向好一些。



# HMM（隐马尔科夫模型）

HMM的训练是由两个二维矩阵得到，发射矩阵和转移矩阵。发射矩阵计算某个tag发射到某个word的概率，转移矩阵计算前一个tag转移到这一个tag的概率。概率由训练语料的频率近似得到。（即极大似然估计方法）

对于参数中很多0的解决办法：使用加 alpha 平滑

预测：使用维特比算法。

注：在维特比算法中对于max_p，可以利用numpy的特性直接一行一行地计算。



# Linear Model（线性模型）

线性模型把词性标注作为一个多分类问题，把句子中的每一个词作为独立的个体。

对于每个词及其对应的词性都会抽取出相应的特征，作为特征模板，全部的特征组合成特征空间。

在加入部分特征优化后，特征模板不再包含tag的信息，也就意味着一个特征对应着全部tag，广义上扩大了特征空间，特征空间不仅包含训练集中抽取的正特征，还包含训练集中不存在的副特征。但这部分副特征在测试集中可能存在，故而有一定效率的提升。

在部分特征优化版本中，对于权重矩阵的设计为特征空间数*tag数，这样在计算分数时方便使用numpy的性质直接对全部的tag的分数进行计算。

使用时间戳延迟对v的更细，只有当w更新后，这个位置的v才进行更新，有一定的速度提升。



# Log Linear Model（对数线性模型）

对数线性模型同样把词性标注作为多分类问题，但是与线性模型不同的是，对数线性模型对于权重的更新不再是仅仅使用简单的对应特征位置上权重的加减，而是使用随机梯度下降的方法累加每一个梯度，到一个batch时更新权重。

L2正则化：为了减小过拟合而加入一个正则项来约束模型参数，在本模型中效果不佳。

模拟退火：对补偿进行调整以适应收敛速率。

对于梯度的储存设计，由于更新的部分是少量的，所以采用默认字典的方式来储存梯度，这样的效率要比直接用与权重相同维度的矩阵来存储梯度高得多，特别是在采用部分特征优化后权重矩阵特别大的时候。

在部分特征优化版本中，在更新一个词带来的梯度变化时，直接算出这个词的特征相对于每个tag的分数，然后求出每个tag的对应的概率，更新到梯度上，利用numpy的特性，效率很高。



# Global Linear Model（全局线性模型）

全局线性模型把词性标注作为序列标注问题，整体方法和线性模型一样，只是线性模型把句子中的每一个作为独立的个体，而全局线性模型是把一个句子作为一个整体，使用Viterbi 解码来预测整个词性序列。序列标注相比于单独对每个词进行词性标注的优势在于它保存了前后词性的信息，所以总体上有正确率的提升。

全局线性模型中，将unigram和bigram分开是比较重要的一点，特别是在特征优化版本中。在小数据上测试，分开后约10s/迭代，而不分开约1min7s/迭代。将bigram分开后，每句预测时只需要计算一次bigram_score，然后再与每个词计算出来的unigram_score与delta相加，得到下一个词的path和delta。

在使用步长优化时，对步长进行指数式衰减，learn_rate = eta * decay_rate ** (global_step / decay_steps)。eta为初始的学习速率，global_step为全局step，与decay_steps和decay_rate一起决定了learn_rate的变化。decay_steps = 预料大小 / batch_size。



# Conditional Random Field （条件随机场）

条件随机场同样把词性标注作为一个序列标注问题，方法类似于对数线性模型，只是对数线性模型把句子中的每一个作为独立的个体，而条件随机场是把一个句子作为一个整体，用P(Y|S)表示句子S标为序列Y的概率。使用梯度下降的方法求解CRF似然函数。通过forward算法和backward算法分别从前往后和从后往前求出部分路径得分之和，进而求解p(i,t',t|s)，得到一个句子的梯度。